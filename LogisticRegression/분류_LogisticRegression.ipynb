{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀 (분류알고리즘)\n",
    "- 로지스틱 = 로짓이라고 부름\n",
    "- Logisstic Regression (Logit Regression)은 샘플이 특정 클래스에 속할 확률을 추정하는데 널리 사용됨. \n",
    "- 계산법은 입력 특성들의 가중치 합을 계산하고, 편향을 더함 => **선형회귀 처럼 결과값을 바로 출력하지는 않고, 결괏값의 로지스틱 값을 출력함.**\n",
    "- 로지스틱이란 즉 시그모이드 함수(sigmoid function) **최종적으론 시그모이드 값에 대입되서 출력됨.**\n",
    "\n",
    "![logistic1](./LogisticRegression_imgs/logistic1.PNG)\n",
    "\n",
    "![logistic2](./LogisticRegression_imgs/logistic2.jpg)\n",
    "\n",
    "- **클래스 1에 속한 샘플을 정확히 예측하면 비용이 0에 가까워짐. (왼쪽상단에서 오른쪽 하단으로)**\n",
    "- **클래스 0에 속한 샘플을 y=0으로 정확히 예측하면 비용이 0에 가까워짐 (오른쪽상단에서 왼쪽 하단으로)**\n",
    "\n",
    "## sklearn.linear_model.LogisticRegression\n",
    "- penalty : l1, l2, elasticnet, 'none' / default = 'l2'\n",
    "- solver = 'warn' : 항상 지정 필요\n",
    "- C=1.0 : 규제항 값 - 규제값이 커지면 가중치도 커짐.\n",
    "- multi_class='warn' : 이진분류 or 다중분류 , 기본적으로는 auto (자동설정인것)\n",
    "- max_iter = 100\n",
    "- random_state = None\n",
    "\n",
    "- dual=False \n",
    "- fit_intercept = True, \n",
    "- intercept_scailing = 1\n",
    "- n_jobs = None, \n",
    "- tol=0.0001\n",
    "- verbose = 0,\n",
    "- warn_start= False\n",
    "\n",
    "### ** penalty - solver **\n",
    "- 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties.\n",
    "- 'elasticnet' is only supported by the 'saga' solver. \n",
    "- 'l1 penalty' with SAGA solver (allowing ‘multinomial’ + L1)\n",
    "\n",
    "### predict method\n",
    "- predict() : 클레스 레이블을 반환하는 메서드 ( 0보다 클 때를 양성 클래스로 판단하여 결과 반환)\n",
    "- predict_probat() : 클래스에 속할 확률을 반환 (시그모이드 함수 적용하여 계산)\n",
    "\n",
    "### 규제를 이용한 과대적합 피하기\n",
    "- 과대적합 : 분산이 크다 / 모델 파라미터가 너무 많아 주어진 데이터에서 너무 복잡한 모델을 만든다.\n",
    "- 과소적합 : 높은편향, / \n",
    "- **분산에 대한 해석 : 분산은 모델을 여러번 훈련했을 때 특정 샘플에 대한 예측의 일관성(변동성)을 측정한다. 모델이 훈련 데이터의 무작위성에 민감하다는 것**\n",
    "- **편향에 대한 해석 : 편향은 다른 훈련 데이터셋에서 여러번 훈련했을 때 예측이 정확한 값에서 얼마나 벗어났는지 측정, 편향은 무적위성보다 구조적인 에러로 해석**\n",
    "- 좋은 편향-분산 트레이드 오프를 찾는 방법은 규제를 사용하여 모델의 복잡도를 조정하는 것 \n",
    "- 규제는 공선성을 다루거나 데이터에서 잡음을 제거하여 과대적합을 방지할 수 있는 매우 유용한 방법\n",
    "- 과도한 파라미터 값을 제한하기 위해 추가적인 정보(편향)을 주입하는 개념 (default L2 규제)\n",
    "- **규제가 잘 동작하려면 모든 특성이 비슷한 스케일을 가져야 한다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2) (150,)\n"
     ]
    }
   ],
   "source": [
    "# (1) 데이터 로드\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris=datasets.load_iris()\n",
    "X = iris.data[:,[2,3]]\n",
    "y = iris.target\n",
    "print(X.shape, y.shape) # (150,2), (150,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 2) (45, 2) (105,) (45,) \n",
      "\n",
      "[[1.4 0.2]\n",
      " [1.7 0.2]\n",
      " [5.3 2.3]]\n",
      "[[-1.33269725 -1.30728421]\n",
      " [-1.16537974 -1.30728421]\n",
      " [ 0.84243039  1.44587881]] \n",
      "\n",
      "[[-1.33269725 -1.30728421]\n",
      " [-1.16537974 -1.30728421]\n",
      " [ 0.84243039  1.44587881]]\n",
      "[0 0 2]\n"
     ]
    }
   ],
   "source": [
    "# (2) train, test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 계층화 기능 사용 하여서 클래스 레이블 비율 동일하게 하는 것.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y) \n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, '\\n') # 105, 45 비율로\n",
    "\n",
    "# (3) 데이터 표준화 진행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "print(X_train[:3])\n",
    "print(X_train_std[:3], '\\n')\n",
    "# (4) 두 배열 합치기 (np.hstack - 두 배열을 왼쪽에서 오른쪽으로 붙이기 / 두 배열을 위에서 아래로 붙이기)\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "print(X_combined_std[:3]) # 150, 2\n",
    "print(y_combined[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.52213484e-12 3.85303417e-04 9.99614697e-01]\n",
      " [9.93560717e-01 6.43928295e-03 1.14112016e-15]\n",
      " [9.98655228e-01 1.34477208e-03 1.76178271e-17]]\n",
      "[2 0 0] \n",
      "\n",
      "[2 0 0] \n",
      "\n",
      "[[-1.27692475 -1.43838721]] \n",
      "\n",
      "[0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#- 'newton-cg', 'sag' and 'lbfgs' solvers support only l2 penalties.\n",
    "#- 'elasticnet' is only supported by the 'saga' solver. \n",
    "#- 'l1 penalty' with SAGA solver (allowing ‘multinomial’ + L1)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l2',#solver = 'liblinear', # \n",
    "                        multi_class='auto', # 이진분류이거나 solver가 liblinear일 때 ovr로 그 외에는 multinomial\n",
    "                        C=100.0,\n",
    "                        random_state=77 )\n",
    "# logistic regression 학습\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# predict_proba : 훈련샘플이 어떤 클래스에 속할 확률 predict_proba\n",
    "db = lr.predict_proba(X_test_std)\n",
    "print(lr.predict_proba(X_test_std[:3]))\n",
    "print(lr.predict_proba(X_test_std[:3]).argmax(axis=1), '\\n')\n",
    "\n",
    "# predict : 훈련샘플이 어떤 클래스에 속한다는 인덱스\n",
    "print(lr.predict(X_test_std[:3]), '\\n')\n",
    "\n",
    "# 내가 테스트 하고 싶은 데이터가 있다면 2차원 형태로 바꿔줌.\n",
    "print(X_test_std[-1].reshape(1,-1), '\\n')\n",
    "print(lr.predict(X_test_std[-1].reshape(1,-1)), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_model -> 추가 라이브러리 \n",
    "![linear_model1](./LogisticRegression_imgs/linear_model1.PNG)\n",
    "![linear_model1](./LogisticRegression_imgs/linear_model2.PNG)\n",
    "![linear_model1](./LogisticRegression_imgs/linear_model3.PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 추가 공부 필요 내용 -> Lasso, Ridge, L1, L2, Elastic 차이점 조사 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
