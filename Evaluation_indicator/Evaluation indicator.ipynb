{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분류 평가지표\n",
    "- 분류를 수행할 수 있는 기계 학습 알고리즘을 만들고 나면, 그 분류기의 예측력을 검증/평가 해봐야 함.\n",
    "- 모델의 성능을 평가하려면 모델을 생성하기 전부터 애초에 데이터를 학습 세트와 평가세트로 분리해서, 학습 세트로 모델을 만들고 평가 세트로 그 모델의 정확도를 확인하는 절차를 가짐\n",
    "\n",
    "- (1) 정확도 (Accuracy)\n",
    "- (2) 오차행렬 (Confusion Matrix)\n",
    "- (3) 정밀도 (Precision)\n",
    "- (4) 재현율 (Recall)\n",
    "- (5) F1 스코어\n",
    "- (6) ROC AUC   \n",
    "<br/>\n",
    "\n",
    "### (1) Accuracy (정확도)\n",
    "- 정확도 는 실제 데이터에서 예측 데이터가 얼마나 같은지 를 판단하는 지표.\n",
    "- 가장 직관적인 모델 예측 성능을 나타내는 평가지표\n",
    "\n",
    "#### 문제점\n",
    "- 고등학교에 남자 150명 여자 50명 인 학교에서 '남자'라고 만 예측해도 75%\n",
    "- 1년에 비오는 날 365일 중 135일 -> '비가 안온다' 만 예측해도 64%\n",
    "- 캐글 타이타닉에서 생존자 대다수가 여자 -> '생존자 == 여자' 만 예측해도 70% \n",
    "- ==> **이를 보완해줄 새로운 통계 기준들이 등장**    \n",
    "<br/>\n",
    "\n",
    "### (2) Confusion Matrix (혼동행렬, 오차행렬)\n",
    "- 일반적으로 이진분류에서의 성능 지표로 활용\n",
    "\n",
    "#### ■ 이진분류 Confusion Matrix)\n",
    "![ConfusionMatrix](./Evaluation_indicator_img/ConfusionMatrix.png)\n",
    "\n",
    "#### Example 크게 4경우\n",
    "- (TP)정답이 O이고, 여러분의 답안이 O인 케이스 (정답을 정답이라 맞췄다 !!!)\n",
    "- (TN)정답이 X고, 여러분의 답안이 X인 케이스  (오답을 오답이라 맞췄다 !!!)\n",
    "\n",
    "- (FP)정답이 O이고, 여러분의 답안이 X인 케이스 (아.. 잘못알고있었네. )\n",
    "- (FN)정답이 X고, 여러분의 답안이 O인 케이스 (아.. 실수했네.)   \n",
    "\n",
    "#### ■ 다중분류 Confusion Matrix\n",
    "![multi_class_confusion_matrix](./Evaluation_indicator_img/multi_class_confusion_matrix.PNG)\n",
    "- 각각을 독립적으로 보고 설명 및 계산 필요\n",
    "- 공식은 똑같음 !\n",
    "- ex) Accuracy : (TP + TN) / (TN + FP + FN + TP)\n",
    "<br/>\n",
    "\n",
    "### (3) Precision (정밀도 = PPV(Positive Predictive Value))\n",
    "- 정밀도란 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율.\n",
    "- **분류기가 맞다고 하는 것 중**에서 분류기가 몇개나 제대로 분류했는가\n",
    "![precision](./Evaluation_indicator_img/precision.png)\n",
    "<br/>\n",
    "\n",
    "### (4) Recall (재현율 = sensitivity(민감도=TPR) = hit rate)\n",
    "- 재현율이란 실제 True인 것 중에서 모델이 True라고 예측한 것의 비율.\n",
    "- **맞다고 분류해야 하는 것 중**에서, 분류기가 몇개나 제대로 분류했는가\n",
    "![recall](./Evaluation_indicator_img/recall.png)\n",
    "<br/>\n",
    "\n",
    "### (3)-(4) 정밀도-재현율 trade-off \n",
    "- **정밀도는 날씨 예측 모델이 맑다로 예측했는데, 실제 날씨가 맑았는지를 살펴보는 지표**\n",
    "- **재현율은 실제 날씨가 맑은 날 중에서 모델이 맑다고 예측한 비율을 나타낸 지표**\n",
    "\n",
    "- Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우에 관심이 있으나, TP 에 대해 바라보고자 하는 관점만 다름.\n",
    "- Precision 과 Recall 은 서로 반대개념의 지표 이기 때문에 하나가 내려가면 다른 하나는 올라갈 수 밖에 없음.\n",
    "- https://sumniya.tistory.com/26 <- 해당링크에서 잘 설명해주지만 \n",
    "![precision_recall_tradeoff](./Evaluation_indicator_img/precision_recall_tradeoff.png)\n",
    "<br/>\n",
    "\n",
    "- **==> 결론1 : 데이터의 비율(Confusion Matrix)을 보고 Precision과 Recall로 이 데이터의 평가 결과에 대한 각각에 대한 해석이 가능.**   \n",
    "- **==>결론2 : Precision과 recall은 높으면 좋다. 단 서로 상관관계가 있어 같이 올리기가 힘듬**   \n",
    "<br/>\n",
    "\n",
    "#### 전체 공식 정리\n",
    "- 정밀도 <-> 재현율\n",
    "- 특이도와 민감도는 ROC 및 AUC를 구할 때 이용됨.\n",
    "![evaluation_indicator](./Evaluation_indicator_img/evaluation_indicator.PNG)\n",
    "<br/>\n",
    "\n",
    "### (5) F1-Score\n",
    "- Recall과 Precision의 균형값 (조화평균)\n",
    "- **F1 score는 데이터 label이 불균형 구조일 때(Confusion Matrix 값이 다 튈때), 모델의 성능을 정확하게 평가**할 수 있으며, 성능을 하나의 숫자로 표현할 수 있음\n",
    "- **단순 산술평균으로 사용하지 않는 이유 (조화평균의 의미)**\n",
    "![F1-Score0](./Evaluation_indicator_img/F1-Score0.png)\n",
    "<br/>\n",
    "\n",
    "- 기하학적으로 봤을 때, 단순 평균이라기보다는 작은 길이 쪽으로 치우치게 된, 그러면서 작은 쪽과 큰 쪽의 사이의 값을 가진 평균이 도출됨. **조화평균을 이용하면 산술평균을 이용하는 것보다, 큰 비중이 끼치는 bias가 줄어든다고 볼 수 있음.**   \n",
    "\n",
    "![F1-Score2](./Evaluation_indicator_img/F1-Score2.png)\n",
    "<br/>\n",
    "\n",
    "#### ■ 다중클래스에서의 F1-Score (Macro F1 Score / Micro F1 Score)\n",
    "- Multi-class상황에서는 f1 score를 바로 정의할 수 없다. \n",
    "- **Macro F1-Score : 개별적인 class마다의 f1 score를 구하고 이를 모든 class에 대해서 평균을 내면 Macro F1-Score** ==> 각 클래스마다 F1-Score 구하고 평균\n",
    "- **Micro F1-Scroe : 개별적인 class 마다의 f1 score를 구하고 이를 클래스별 imbalance를 고려해서 weighted average를 하면 Micro F1-Score** ==> 각 클래스마다 F1-Score 구하고 가중치 평균 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Fall-out(FPR - False Positive Rate) (1-특이도)\n",
    "- Fall-out은 실제 False인 데이터 중 모델이 True라고 예측한 비율입니다. \n",
    "- 0인 케이스에 대해 1로 잘못 예측한 비율.(암환자가 아닌데 암이라고 진단 함)\n",
    "![Fallout](./Evaluation_indicator_img/Fallout.png)\n",
    "<br/>\n",
    "\n",
    "### (7) Sensitivity (민감도 = Recall)\n",
    "- TPR : True Positive Rate (= 민감도, true accept rate)\n",
    "- 1인 케이스에 대해 1로 잘 예측한 비율.(암환자를 진찰해서 암이라고 진단 함)\n",
    "![sensitivity](./Evaluation_indicator_img/sensitivity.PNG)\n",
    "<br/>\n",
    "\n",
    "### (8) Specificity (특이도)\n",
    "- Specificity는 실제 0인 케이스에 대해 0이라고 예측한 비율 (암환자가 아닌 사람을 진찰해서 아니라고 함)\n",
    "![specificity](./Evaluation_indicator_img/specificity.PNG)\n",
    "<br/>\n",
    "\n",
    "#### Confusion Matrix\n",
    "![ConfusionMatrix](./Evaluation_indicator_img/ConfusionMatrix.png)\n",
    "<br/>\n",
    "\n",
    "### (9) ROC (Receiver Operation Characteristic Curve) (그래프)\n",
    "- **Recall(민감도)와 Fallout의 변화를 시각화환 것**\n",
    "- curve가 왼쪽 위 모서리에 가까울수록 모델의 성능이 좋다고 평가.\n",
    "- **즉, Recall이 크고 Fall-out이 작은 모형이 좋은 모형**\n",
    "- **y=x 그래프보다 상단에 위치해야 어느정도 성능이 있다고 말할 수 있음**\n",
    "![ROC](./Evaluation_indicator_img/ROC.png)\n",
    "<br/>\n",
    "\n",
    "### (10) AUC (Area Under Curve) (ROC 에 대한 수치값)\n",
    "- AUC(Area Under Curve)란 ROC curve 그래프 아래의 면적값\n",
    "- 최대값은 1이며 좋은 모델(즉, Fall-out에 비해 Recall 값이 클수록) 1에 가까운 값이 나옵니다.\n",
    "<br/>\n",
    "\n",
    "#### 해석\n",
    "- **==> 결론1 : 데이터의 비율(Confusion Matrix)을 보고 Recall과 Fall-out에 대한 각각에 대한 해석이 가능.**   \n",
    "- **==>결론2 : Recall과 Fall-out을 통해 그린 AUC Curve로 확인, 면적이 크면 좋다.\n",
    "<br/>\n",
    "\n",
    "### 참고링크\n",
    "- https://subinium.github.io/basic-of-Evaluation/ // 분류지표들\n",
    "- https://sumniya.tistory.com/26 // 분류지표들 2\n",
    "- https://neosla.tistory.com/18 // 다중 Confusion Matrix\n",
    "- http://hleecaster.com/ml-accuracy-recall-precision-f1/ // python 코드\n",
    "- https://newsight.tistory.com/53 [New Sight] // 다중 F1-Score (Micro, Macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.3\n",
      "recall_score 0.42857142857142855\n",
      "precision_score 0.5\n",
      "f1_score 0.4615384615384615\n",
      "\n",
      "confusion_matrix\n",
      " [[0 3]\n",
      " [4 3]]\n",
      "roc_curve 면적 0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "# 분류 평가 지표\n",
    "\n",
    "# Accuracy, Precision, Recall, F1-Score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "y_test = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]\n",
    "pred = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# score 출력\n",
    "print('accuracy_score', accuracy_score(y_test, pred))\n",
    "print('recall_score', recall_score(y_test, pred))\n",
    "print('precision_score', precision_score(y_test, pred))\n",
    "print('f1_score', f1_score(y_test, pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('\\nconfusion_matrix\\n', confusion_matrix(y_test, pred))\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc = roc_auc_score(y_test, pred)\n",
    "print('roc_curve 면적', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.50      0.29         2\n",
      "           1       0.50      0.25      0.33         4\n",
      "           2       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.50        10\n",
      "   macro avg       0.57      0.50      0.49        10\n",
      "weighted avg       0.64      0.50      0.53        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다중분류 코드\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 다중분류인경우\n",
    "y_test2 = [1, 2, 0, 1, 2, 1, 0, 1, 2, 2]\n",
    "pred2 = [0, 2, 0, 1, 2, 0, 1, 0, 2, 0]\n",
    "print(classification_report(y_test2,pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회귀 평가지표\n",
    "- **회귀의 평가를 위한 지표는 실제 값과 회귀 예측값의 차이를 기반**으로 함.\n",
    "- **회귀 평가지표 MAE, MSE, RMSE, MSLE, RMSLE는 값이 작을수록 값이 예측값과 실제값의 차이가 없다는 뜻으로 회귀 성능이 좋은 것**\n",
    "- 반면, R² 는 값이 클수록 성능이 좋습니다.\n",
    "\n",
    "- (1) MAE (Mean Absolute Error)\n",
    "- (2) MSE (Mean Squared Error)\n",
    "- (3) MSLE (Mean Squared Log Error)\n",
    "- (4) RMSE (Root Mean Squared)\n",
    "- (5) MAPE (Mean Absolute Percentage Error)\n",
    "- (6) MSLE (Mean Squared Log Error)\n",
    "- (7) RMSLE (Root Mean Squared Log Error)\n",
    "- (8) R² (R Sqaure)\n",
    "\n",
    "### (1) MAE(Mean Absolute Error)\n",
    "- 실제 값과 예측 값의 차이를 절댓값으로 변환해 평균한 것\n",
    "![mae](./Evaluation_indicator_img/mae.png)\n",
    "\n",
    "### (2) MSE(Mean Squared Error)\n",
    "- 실제 값과 예측 값의 차이를 제곱해 평균한 것\n",
    "![mse](./Evaluation_indicator_img/mse.png)\n",
    "\n",
    "### (3) MSLE (Mean Squared Log Error)\n",
    "- MSE에 로그를 적용해준 지표입니다. log(y)가 아니라 log(y+1)입니다. y=0일 때, log(y)는 마이너스 무한대이므로 이를 보정해주기 위해 +1을 한 것입니다. \n",
    "![msle](./Evaluation_indicator_img/msle.png)\n",
    "\n",
    "### (4) RMSE (Root Mean Squared)\n",
    "- MSE 값은 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있어 MSE에 루트를 씌운 RMSE 값을 쓰는 것입니다.\n",
    "![rmse](./Evaluation_indicator_img/rmse.png)\n",
    "\n",
    "#### 제한사항 (크기 의존적 에러 - Scale-dependent Errors)\n",
    "- 삼성전자 주가 2,389,000 / Naver 778,000 \n",
    "- RMSE 500,000 / 500,000\n",
    "- 모델이 해당 케이스에 동일한 성능일까? ==> 삼성잔자의 500,000와 네이버의 500,000은 다르다\n",
    "\n",
    "### (5) MAPE (Mean Absolute Percentage Error)\n",
    "- MAPE는 크기 의존적 에러의 단점을 커버하기 위한 모델.\n",
    "![MAPE](./Evaluation_indicator_img/MAPE.PNG)\n",
    "- At는 실제값 , Ft는 예측값 \n",
    "- MAPE는 예측값과 실제 값을 뺀 후 이를 실제 값으로 나눈 값을 모두 더한 후 n으로 나누고 백분율 표현을 위한 100곱하기.\n",
    "- 문제1) At 실제 값이 1보다 작을 떄 발생. 실제 값이 1보다 작기 때문에, MAPE는 무한대에 가까운 값을 찍을 수 있음.\n",
    "- 문제2) At 실제 값이 0이라면, MAPE 값 자체를 계산 불가\n",
    "\n",
    "### (6) MASE (Mean Absolute Percentage Error)\n",
    "- MASE는 예측 값과 실제 값의 차이를 평소에 움직이는 평균변동폭으로 나눈 값\n",
    "- MAPE가 오차를 실제 값으로 나누었지만, MASE는 평소에 변동폭에 비해 얼마나 오차가 나는지를 측정하는 기준\n",
    "- ==> MASE는 변동성이 큰 지표와 변동성이 낮은 지표를 같이 예측할 필요 있을때 유용하게 쓰임\n",
    "\n",
    "![MASE](./Evaluation_indicator_img/MASE.PNG)\n",
    "\n",
    "### (7) RMSLE (Root Mean Squared Log Error)\n",
    "- RMSE에 로그를 적용해준 지표입니다.\n",
    "![rmsle](./Evaluation_indicator_img/rmsle.jpg)\n",
    "- RMSLE 는 과대 평가된 항목보다는 과소평가 된 항목에 패널티를 주고,\n",
    "- 0에 가까운 값이 나올 수록 정밀도가 높은 값\n",
    "\n",
    "### (8) R² (R Sqaure) (=결정계수)\n",
    "- Regression model의 성능을 측정하기 위해 **mean value로 예측하는 단순 모델과 비교, 상대적으로 얼마나 성능이 나오는지를 측정**\n",
    "- 다른 성능 지표인 RMSE나 MAE는 데이터의 scale에 따라서 값이 천차만별이기 때문에, 절대 값만 보고 바로 판단하기가 어려움.\n",
    "- R^2의 단점은 데이터가 매우 단순하여 평균을 예측하는 것만으로도 오차가 충분히 작을 때 부정확한 측정을 할 수 있다는 점일 것이다\n",
    "\n",
    "### RMSE와 비교해서 RMSLE가 가진 장점\n",
    "### 1. 아웃라이어에 강건해진다.\n",
    "- RMSLE는 아웃라이어에 강건(Robust)합니다.\n",
    "\n",
    "- 예측값 = 67, 78, 91, 실제값 = 60, 80, 90일 때, RMSE = 4.242, RMSLE = 0.6466입니다.\n",
    "- 예측값 = 67, 78, 91, 102, 실제값 = 60, 80, 90, 750일 때 RMSE = 374.724, RMSLE = 1.160\n",
    "- 750이라는 아웃라이어 때문에 RMSE는 굉장히 증가했지만 RMSLE의 증가는 미미합니다.\n",
    "- RMSLE는 아웃라이어가 있더라도 값의 변동폭이 크지 않습니다. \n",
    "\n",
    "### 2. 상대적 Error를 측정해준다.\n",
    "- 예측값과 실제값에 로그를 취해주면 로그 공식에 의해 아래와 같이 상대적 비율을 구할 수 있습니다.\n",
    "\n",
    "- 예측값 = 100, 실제값 = 90일 때, RMSLE = 0.1053, RMSE = 10입니다.\n",
    "- 예측값 = 10,000, 실제값 = 9,000일 때, RMSLE = 0.1053, RMSE = 1,000입니다.\n",
    "- 보시는 바와 같이 값의 절대적 크기(단위)가 변하더라도 상대적 크기가 동일하다면 RMSLE 값도 동일합니다. \n",
    "\n",
    "### 3. Under Estimation에 큰 패널티를 부여한다.\n",
    "- RMSLE는 Over Estimation보다 Under Estimation에 더 큰 패널티를 부여합니다. 즉, 예측값이 실제값보다 클 때보다 예측값이 실제값보다 작을 때 더 큰 패널티를 부여합니다.\n",
    "\n",
    "- 예측값 = 600, 실제값 = 1,000일 때 RMSE = 400, RMSLE = 0.510입니다.\n",
    "- 예측값 = 1,400, 실제값 = 1,000일 때 RMSE = 400, RMSLE = 0.33입니다.\n",
    "- 예측값과 실제값의 차이는 둘 다 400입니다. Over Estimation이든 Under Estimation이든 RMSE값은 동일합니다. 하지만 RMSLE는 Under Estimation일 때 (즉, 예측값이 실제값보다 작을 때) 더 높은 페널티가 주어집니다. \n",
    "- 배달 음식을 시킬 때 30분이 걸린다고 했는데 실제로 20분이 걸리는 건 큰 문제가 되지 않지만, 30분이 걸린다고 했는데 40분이 걸리면 고객이 화가 날 수 있습니다. 이럴 때 RMSLE를 적용할 수 있습니다.\n",
    "\n",
    "### 참고링크\n",
    "- https://brunch.co.kr/@chris-song/34 \n",
    "- https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-17-%ED%9A%8C%EA%B7%80-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45454545454545453\n",
      "0.45454545454545453\n",
      "0.674199862463242\n",
      "0.029272467607503516\n",
      "0.1710919858073531\n",
      "0.868421052631579\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error\n",
    "\n",
    "origin = np.array([1, 2, 3, 2, 3, 5, 4, 6, 5, 6, 7])\n",
    "pred = np.array([1, 1, 2, 2, 3, 4, 4, 5, 5, 7, 7])\n",
    "\n",
    "\n",
    "MAE =  mean_absolute_error(origin, pred)\n",
    "print(MAE)\n",
    "\n",
    "MSE = mean_squared_error(origin, pred)\n",
    "print(MSE)\n",
    "\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "MSLE = mean_squared_log_error(origin, pred)\n",
    "print(MSLE)\n",
    "\n",
    "RMSLE = np.sqrt(mean_squared_log_error(origin, pred))\n",
    "print(RMSLE)\n",
    "\n",
    "R2 = r2_score(origin, pred)\n",
    "print(R2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
