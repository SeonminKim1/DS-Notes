{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 앙상블 (ensemble)\n",
    "- 앙상블 쓰는 이유 : 여러 개의 단일 모델들의 평균치를 내거나, 투표를 해서 다수결에 의한 결정을 하는 등  **더 나은 결과를 도출해 내기 위한 목적기법**\n",
    "- **우리는 데이터를 통해 모델링을 하고 결국에는 새로운 데이터가 들어왔을 때, 예측이나 분류를 해주길 원하고, 모델은 학습 데이터에 너무 오버피팅 되면 안된다. 그것을 막기 위해 약한 모델을 여러개 결합시켜 그 결과를 종합한다가 앙상블의 기본 아이디어**\n",
    "- 부스팅은 기본 앙상블 개념에 Sequential (순차적인) 이 추가된것. 연속적인 모델의 weak learner, 즉 바로 직전의 weak learner의 error를 반영하여 바로잡겠다가 아이디어.\n",
    "- 배깅방법과 부스팅 방법이 있고, 배깅에는 RandomForest / 부스팅방법에는 AdaBoost, GradientBoost(GBM), XGBoost, LightGradientBoost 등이 있음.\n",
    "\n",
    "### 1. 배깅(Bagging = Bootstrap Aggregating = 각각의 부트스트랩 샘플로부터 합침)   \n",
    "- 샘플을 여러번 뽑아 각 모델을 학습시켜 결과를 집계\n",
    "- 복원추출로 N개의 데이터셋을 만들고 N개의 모델을 만듬\n",
    "- 다양한 데이터 셋 사용 하니까 Overfitting 방지\n",
    "- Classification과 regression 모두 사용 가능\n",
    "![배깅](./Bagging_Boosting_imgs/배깅.png)\n",
    "\n",
    "### 1 - (1) 랜덤 포레스트\n",
    "- https://dive-into-ds.tistory.com/9\n",
    "- 여러 개의 의사 결정 나무를 만들고 그들의 다수결로 결과를 결정하는 방법\n",
    "- 여러 개의 training data를 생성하여 각 데이터마다 개별 의사 결정나무모델 구축 (Bagging)\n",
    "- 의사결정 나무모델 구축시 변수 무작위로 선택 (Random subspace) \n",
    "- Random subspace에서의 변수 선택의 갯수는 일반적으로 전체변수의 갯수의 제곱근\n",
    "- 원래 변수의 수보다 적은수의 변수를 임의로 선택하여, 해당 변수들만을 고려대상으로 함.\n",
    "- 성능이 좋고, 함수가 간단해서 하이퍼 파라미터 튜닝 없이 간단히 쓰기 가능\n",
    "\n",
    "- **Bagging과 Random Subspace 기법은 각 모델들의 독립성, 일반화, 무작위성을 최대화시켜 모델간의 상관관계 p를 감소시키고**, 개별 tree의 정확도, 독립성이 높을수록 Random forest의 성능이 높아짐.\n",
    "![rf](./Bagging_Boosting_imgs/rf.png)    \n",
    "\n",
    "### 2. Boosting\n",
    "- 배깅의 변형으로 모델이 잘 예측하지 못하는 부분을 개선하기 위한 모델\n",
    "- 이전 모델들이 예측하지 못한 Error data에 가중치를 부여하여, 다음 모델이 더 잘 예측되도록 하는 것.\n",
    "![부스팅](./Bagging_Boosting_imgs/부스팅.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2 - (1) AdaBoost (Adaptive Boosting) (1997)\n",
    "- https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting // 출처 및 15쪽\n",
    "- 1997년 최초의 부스팅 알고리즘\n",
    "- 가중치를 부여한 약 분류기를 모아서 최종적인 강 분류기를 생성하는 기법\n",
    "- (1) 전체 데이터에서 random sampling / (2) 모든 sample 데이터 가중치 초기화 / (3) 첫번째 weak learner를 만들고 학습 후 결과에 따라 각 데이터에 가중치와 weak learner의 모델 가중치 출력 / (4) 3번에서 구한 데이터 가중치로 데이터를 업데이트 / (5) update된 데이터로 두번째 weak learner를 만들고 학습한다. (3)~(5)과정 반복\n",
    "- Adaboost의 경우, 다음 학습 과정에서  각각의 샘플들에 대해 가중치를 두고 모델링을 하여 다음 학습시 이전에 처리하지 못한 샘플들을 adaptive하게 더 잘 풀도록 개선하는 특성이 있다. 그래서 Adaptive Boosting 인 것\n",
    "- 훈련 과정에서 모델 능력을 향상시키는 특성들을 골라 선택하며, 이를 통해 실시간 처리에서 속도 개선을 돕는다.   \n",
    "\n",
    "### 2 - (2) GradientBoost\n",
    "- https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting // 출처 및 20쪽\n",
    "- Sequential 한 Weak learner들을 residual(잔차)를 줄이는 방향으로 결합하여 loss를 줄여나가는 아이디어. \n",
    "- **AdaBoost와 기본 개념은 동일하고, 가중치를 계산하는 방식에서 Gradient Descent를 이용하여 최적의 파라미터를 찾아냄**\n",
    "- Boosting은 과적합 가능성이 높으므로, 적절한 시점에 멈춰야 한다.\n",
    "- GBM은 residaul을 줄이는 방향으로 weak learner를 결합해 강력한 성능을 자랑하지만, 해당 train data에 residual을 계속 줄이니까 overfitting 되기 쉽다는 문제점이 있다.   \n",
    "\n",
    "### 2 - (3) XGBoost (2014)\n",
    "- https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting // 출처 및 22쪽\n",
    "- https://hyunlee103.tistory.com/25 // 출처2\n",
    "- 자체 과적합 규제 기능으로 과적합에 좀 더 강한 내구성을 가질 수 있음.\n",
    "- GBM 보다 속도는 빠르지만, GBM 보다만 빠른 것 \n",
    "- 다양한 loss function지원을 통한 데이터에 대한 유연한 튜닝 가능\n",
    "- 분산 / 병렬 처리 가능\n",
    "- Binary classification / Multiclass Classification / Regression / Learning to Rank 형식 모델 지원\n",
    "- **XGBoost = GBM + Regularization + 다양한 Loss Function + 분산/병렬처리 **\n",
    "\n",
    "### 2 - (4) LightGradientBoost  (2016)\n",
    "- GBM 기반 / Ranking, classification 등의 문제에 활용\n",
    "- **대량의 데이터를 병렬로 빠르게 학습가능 (Low memory, GPU 활용가능 / 예측정확도가 더 높음 / xgboost 에 비해 과적합에 보다 민감 = 확률 높음)**\n",
    "- XGBoost대비 2~10배 빠름 \n",
    "- **Leaf-wise로 tree를 성장 (수직방향) 다른 알고리즘 (Level-wise) 와 다름**\n",
    "![leafwise_levelwise](./Bagging_Boosting_imgs/leafwise_levelwise.jpg)    \n",
    "\n",
    "### 부스팅 결론\n",
    "\n",
    "#### Adaboost\n",
    "- 다수결을 통한 오답에 가중치 부여\n",
    "\n",
    "#### GBM\n",
    "- Loss Funtion에 Gradient Descent 방식을 적용하여 오답에 가중치 부여\n",
    "\n",
    "#### XGboost\n",
    "- GBM의 성능 향상 version \n",
    "- Regularization -> Fast + Accurate\n",
    "- Kaggle 다수 수상작\n",
    "\n",
    "#### Light GBM\n",
    "- XGboost.GBM 대비 성능 향상 및 자원 소모 최소화\n",
    "- huge dataset에 적합\n",
    "\n",
    "![boosting](./Bagging_Boosting_imgs/boosting.png)    \n",
    "\n",
    "\n",
    "### 3. 배깅 vs 부스팅\n",
    "- Bagging은 일반적인 모델을 만드는데 집중되어 있는 반면, Boosting은 맞추기 어려운 문제를 맞추는데 초점이 맞춰져 있음.\n",
    "- Boosting은 속도가 느리지만 Bagging에 비해 에러가 적음.\n",
    "- Boosting은 데이터를 사용한 걸 재사용하니 오버피팅 될 가능성이 있음.\n",
    "- ==> **개별 결정 트리의 낮은 성능이 문제라면 Boosting / 오버피팅이 문제라면 Bagging**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.06.28 추가내용\n",
    "\n",
    "### 1. 배깅 (Bootstrap Aggregating)\n",
    "- 부트스트랩 방식으로 데이터를 구성한다. (중복을 허용하여 뽑는다) \n",
    "- 개별 분류기가 부트스트랩 샘플에 학습되고 나면 다수결 투표를 사용하여 예측을 모은다.\n",
    "- 실전에서 고차원 데이터셋을 사용하는 더 복잡한 분류 문제라면 단일 결정 트리가 쉽게 과대적합 될 수 있다.\n",
    "- 배깅 알고리즘은 모델의 분산을 감소시킨다\n",
    "- 모델의 편향을 낮추는 데는 효과적이지 않다.\n",
    "- 즉 모델이 너무 단순해서 데이터에 있는 경향을 잘 잡아 내지 못하고, 이것이 배깅을 수행할 때 편향이 낮은 모델, 예를 들어 가지치기 하지 않은 결정 트리를 분류기로 사용하여 앙상블을 만드는 이유\n",
    "\n",
    "### 2. 부스팅 \n",
    "- 훈련 세트에서 중복을 허용하지 않고 두번째 랜덤한 데이터를 뽑고, 이전에 잘못 분류된 샘플의 50%를 더해서 약한 학습기를 훈련하는 방식.\n",
    "- 배깅모델에 비해 분산 및 편향도 감소시킬수 있으나, 과대적합 되는 경향이 있음.\n",
    "- 서로 다른 가중치가 부여된 훈련세트에서 훈련된 3개의 약한 학습기를 다수결 투표 방식으로 합침\n",
    "- 부스팅 알고리즘 중 인기 있는 다른 알고리즘은 그래디언트 부스팅(Gradient Boosting)\n",
    "- Adaboost와는 달리 이전의 약한 학습기가 만든 잔차 오차에 대해 학습하는 새로운 학습기를 추가하는 것.\n",
    "- 사이킷런에 조기 종료 기능도 사용가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
